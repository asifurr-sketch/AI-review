# Metadata

**Category:** - Coding

**Topic:** - Competitive Programming

**Subtopic:** - ["Arrays and Lists", "Searching Algorithms", "Divide and Conquer", "Sorting Algorithms", "Algorithm Complexity and Big O Notation"]

**Difficulty:** - Hard

**Languages:** - Python

**Number of Approaches:** - $3, (O(R_r * R_s * n) -> O(R * n) -> O(N log R + N log N))$

**Number of Chains:** - 11

---

**[User]**

*Demark the start of the User's activity*

---

**[Prompt]**

**Glowkeeper’s Even Ramp**5

Time Limit: **5 seconds**

Memory Limit: **128 MB**

For the Lantern Parade, a Glowkeeper must line a staff with light-beads. Each bead already shines with some integer glow, and repainting one unit of glow costs a bead-specific price. The parade’s director insists the final glows follow a perfect ramp: the 0th bead has glow $s$, the 1st has $s + r$, the 2nd has $s + 2r$, and so on. The Glowkeeper may choose both the starting glow $s$ and the per-bead glow increase $r$, but must obey this order of fairness:
1) First, cap how bad the single worst repaint is across all beads.
2) Next, among those choices, minimize the total weighted repaint effort.
3) Finally, if there is still a tie, prefer the lexicographically smallest final glow sequence (compare bead 0, then bead 1, etc.).

You are given:
- an integer array $x[0..n-1]$ of current glows, and
- an integer array $p[0..n-1]$ of strictly positive repaint costs per unit for each bead.

Choose integers $s$ and $r$ and define the final sequence $y[i] = s + r \cdot i$ for $0 \le i < n$. Optimize in this exact priority:
1) Minimize $M = \max_i |x[i] - y[i]|$.
2) Among all $(s, r)$ achieving that minimal $M$, minimize $C = \sum_i p[i] \cdot |x[i] - y[i]|$.
3) Among those, choose the lexicographically smallest sequence $y$; for arithmetic progressions this is equivalent to minimizing $s$, then $r$ if $s$ ties.

The order of beads is fixed; you may not reorder them.

**Input Format:-**

- The first line contains an integer $t$, the number of test cases.
- For each test case:
  - The first line contains an integer $n$.
  - The second line contains $n$ integers $x_0, x_1, \dots, x_{n-1}$.
  - The third line contains $n$ integers $p_0, p_1, \dots, p_{n-1}$, with $p_i \ge 1$.

**Output Format:-**

For each test case:
- Print two integers on the first line: the minimized worst repaint $M$ and, under that $M$, the minimized total weighted cost $C$.
- Print the resulting sequence $y$ on the next line: $n$ integers $y_0, y_1, \dots, y_{n-1}$.

**Constraints:-**

- $1 \le t \le 100$
- $1 \le n$, and the sum of $n$ over all test cases $\le 2 \cdot 10^5$
- $|x_i| \le 10^9$
- $1 \le p_i \le 10^9$
- $s$ and $r$ can be any signed 64-bit integers
**Examples:-**
 - **Input:**
```
1
5
0 0 10 10 10
20 1 1 1 1
```

 - **Output:**
```
4 10
0 3 6 9 12
```

 - **Input:**
```
1
6
4 4 4 5 5 5
1 2 3 3 2 1
```

 - **Output:**
```
1 6
4 4 4 4 4 4
```

**Note:-**
In the first example, we must first minimize the worst repaint $M$. Trying $M=3$ is infeasible:
- From $i=1$ and $i=4$: $s+r \in [-3,3]$ and $s+4r \in [7,13] \Rightarrow 3r \in [10,16] \Rightarrow r \in \{\!4,5\!\}$.
- Check $r=4$: from $i=0$, $s \in [-3,3]$; from $i=1$, $s \in [-7,-1]$; from $i=2$, $s \in [-1,5]$; from $i=4$, $s \in [-9,-3]$. Intersections force $s=-1$ from the first three, but then $s \notin [-9,-3]$, contradiction.
- Check $r=5$: from $i=1$, $s \in [-8,-2]$; intersecting with $s \in [-3,3]$ (from $i=0$) gives $s \in [-3,-2]$, but from $i=4$, $s \in [-13,-7]$, contradiction.
Hence $M \ge 4$. For $M=4$, taking $s=0$ gives the feasible interval for $r$:
- $|0-r| \le 4 \Rightarrow r \in [-4,4]$,
- $|10-2r| \le 4 \Rightarrow r \in [3,7]$,
- $|10-3r| \le 4 \Rightarrow r \in [2,\tfrac{14}{3}]$,
- $|10-4r| \le 4 \Rightarrow r \in [\tfrac{3}{2},\tfrac{7}{2}]$.
Intersecting yields $r=3$. The resulting ramp is $y=(0,3,6,9,12)$ with
$$M=\max(|0-0|,|0-3|,|10-6|,|10-9|,|10-12|)=4,$$
and
$$C=20\cdot 0+1\cdot 3+1\cdot 4+1\cdot 1+1\cdot 2=10.$$
Among all pairs achieving $M=4$, $s=0$ also minimizes the weighted cost because bead $0$ has weight $20$ (much larger than the sum of the others), and there is no tie in $y$ to break.

In the second example, $M=0$ is impossible (the given $x$ is not an arithmetic progression). With $M=1$:
- From $i=0$: $s \in [3,5]$; from $i=5$: $s+5r \in [4,6] \Rightarrow 5r \in [4-s,6-s] \subseteq [-2,3] \Rightarrow r=0$.
- With $r=0$, feasibility gives $s \in [4,5] \Rightarrow s \in \{4,5\}$.
Both $(s,r)=(4,0)$ and $(5,0)$ achieve $M=1$ and have the same weighted cost
$$C=1\cdot 0+2\cdot 0+3\cdot 0+3\cdot 1+2\cdot 1+1\cdot 1=6.$$
By lexicographic tie-breaking, choose the smaller $s$, giving the constant ramp $y=(4,4,4,4,4,4)$.

---

**[Assistant]**

Demark the start of the Assistant's activity

---

**[CHAIN_01]**

Dissecting the objectives and constraints precisely

**[THOUGHT_01_01]**


- For sequences with identical maximum deviation $M$ and weighted cost $C$, there may exist multiple feasible arithmetic progressions $y$. To determine the lexicographically smallest sequence, we analyze how $s$ and $r$ jointly influence $y[i] = s + r \cdot i$.  

- A smaller $s$ decreases the first element of $y$, and for equal $s$, a smaller $r$ produces slower growth in $y$, which can affect subsequent elements. Stepwise reasoning considers these dependencies to formalize tie-breaking.


**[THOUGHT_01_02]**


- All decision variables $s$, $r$, and computed $y[i]$ are integers. The analysis must account for large magnitudes of $x[i]$ and $i$, but the reasoning focuses on how differences $|x[i]-y[i]|$ accumulate in $M$ and $C$, independent of numeric representation.  

- This establishes that any candidate solution must satisfy $M$ and $C$ criteria before considering practical numeric handling.

**[THOUGHT_01_03]**

- The objectives form a hierarchical structure: first, minimizing the Chebyshev deviation $M$ ensures no single element is excessively far from its target; second, minimizing the weighted sum $C$ favors reducing distance at high-cost indices; third, lexicographic order further discriminates among tied sequences.  

- This ordering emerges from analyzing how each metric dominates the previous in evaluation priority.




**[THOUGHT_01_04]**

- Fixing $r$ reduces the problem: define $d_i(r) = x[i] - r \cdot i$.  
- Then $|x[i] - y[i]| = |d_i(r) - s|$.  
- For a given integer $r$, the Chebyshev objective and the weighted-L1 objective become properties of the multiset $\{d_i(r)\}$ with respect to the choice of integer $s$.



**[THOUGHT_01_05]**

- For a fixed slope $r$, we examine all feasible $s$ that satisfy the minimal $M$. Among these $s$ values, we analyze the effect of each on the sequence $y$.  

- By comparing $y$ element-wise for candidate $s$ values, we determine the lexicographically smallest $y$, ensuring decisions follow from structural analysis rather than assumption.



---

**[CHAIN_02]**

Designing thorough and adversarial test cases

**[THOUGHT_02_01]**

- For $n=1$, the single element sequence constrains $y[0]$ to equal $x[0]$ to minimize deviation. Analytical reasoning shows that no alternative $y$ can reduce $M$ or $C$; thus, the solution structure is uniquely determined from the problem constraints.  




**[THOUGHT_02_02]**



- If $x[i]$ forms an exact arithmetic progression, then any arithmetic progression $y$ matching $x$ yields zero maximum deviation $M$ and weighted distance $C$.  

- This follows from the definition of arithmetic progression and absolute deviation, rather than assumed optimality.  

**[THOUGHT_02_03]**

- When weights $p[i]$ vary significantly, the contribution of each index to $C$ differs. Analyzing feasible $s$ and $r$, we see that elements with higher weights have a larger impact on $C$.  

- Therefore, minimizing $C$ under a fixed $M$ requires prioritizing alignment of $y[i]$ with $x[i]$ at high-weighted indices. This reasoning arises from decomposing $C$ into weighted contributions.

**[THOUGHT_02_04]**



- Consider sequences where most weights $p[i]$ are equal and a single weight $p[j]$ is much larger. The weighted $L1$ objective $C(s)$ assigns greater influence to elements with larger weights. Therefore, for any feasible $s$ under the minimal Chebyshev deviation $M$, the optimal sequence $y$ will align more closely with $x[j]$ than with other elements.

**[THOUGHT_02_05]**

- Consider sequences with extreme element values, e.g., $x[i]$ spanning negative and positive magnitudes. The spread $S(r)=maxdi(r)−mindi(r)$ determines the minimal Chebyshev deviation $M=⌈S(r)/2⌉$. Analytically, differences $di(r)$ and feasible intervals for s remain well-defined, independent of numeric magnitude.

**[THOUGHT_02_06]**


- For sequences with identical $x[i]$ values, any slope $r$ produces equal differences $|x[i]-y[i]|$. Analysis of this symmetry shows that the feasible interval for $s$ is centered around the repeated $x[i]$, and the choice of $s$ within this interval can be evaluated analytically for minimizing $C$ without assuming $r=0$ as automatically optimal.

**[THOUGHT_02_07]**

- Short examples can be constructed where two distinct arithmetic progressions produce the same maximum deviation $M$ and weighted sum $C$, such as symmetric small sequences. Analysis of these examples shows that multiple sequences may satisfy both objectives, requiring consideration of tie-breaking rules based on lexicographic order of $y$, which correspond to $s$ and $r$ ordering.

**[THOUGHT_02_08]**


- Consider sequences x where the spread between $di(r)$ values is an odd integer. In such cases, the Chebyshev bound becomes $M=⌈S/2⌉$, illustrating how integer rounding affects feasible $s$ values. Analysis of adjacent integer $r$ values demonstrates how small changes in slope impact the spread $S(r)$ and the resulting minimal worst repaint, highlighting sensitivity to rounding and parity.

---

**[CHAIN_03]**

Implementing brute force

**[THOUGHT_03_01]**

- A naive plan would try all integer $r$ in some range and all integer $s$ in some range. For each $(s, r)$, compute $M = \max_i |x[i] - (s + r \cdot i)|$ and $C = \sum_i p[i] \cdot |x[i] - (s + r \cdot i)|$, then choose according to the priority order. This requires $O(n)$ work per pair.


**[THOUGHT_03_02]**

- Even if we restrict $r$ to a very wide range like $[-2 \cdot 10^9, 2 \cdot 10^9]$ (since $x[i]$ are up to $10^9$ and $i$ up to $2 \cdot 10^5$ across tests), enumerating is hopeless. Similarly, $s$ cannot be safely bounded by $[\min x, \max x]$ because large $|r|$ and large indices can push the optimal $s$ far outside that interval.


**[THOUGHT_03_03]**

- Suppose we attempt $R_r$ slopes and $R_s$ intercepts; total work is $O(R_r \cdot R_s \cdot n)$, which is intractable for any realistic $R_r, R_s$ when $n$ can be $2 \cdot 10^5$. Even with grossly reduced ranges, we cannot guarantee correctness.


**[THOUGHT_03_04]**

- This approach fails due to the infinite decision space and the lack of safe, small bounds on $s$ and $r$. We need structural properties that let us avoid exploring most $(s, r)$ pairs.


---

**[CHAIN_04]**

Attempting linear slope scans and pairwise candidate generation

**[THOUGHT_04_01]**

- A slightly better idea is to scan $r$ over a large integer interval and, for each $r$, compute the optimal $s$ and the resulting $(M, C)$. However, a range like $[-2 \cdot 10^9, 2 \cdot 10^9]$ still makes this impossible because each $r$ costs $O(n)$ to evaluate, leading to $O(R \cdot n)$ with $R$ astronomically large.


**[THOUGHT_04_02]**


- Another idea is to consider slopes induced by pairs, e.g., $r \approx (x[j]-x[i])/(j-i)$. For integer $r$, we can consider the floor and ceiling of this ratio. However, generating $O(n^2)$ such candidates remains infeasible for large $n$.  

**[THOUGHT_04_03]**

- Even if we somehow enumerated all near-critical slopes, we would still need to compute $s$ minimizing the weighted $L_1$ cost under the first-layer $M$-optimality constraint. Naively, that adds an $O(n \log n)$ sorting per candidate slope, compounding the infeasibility.


**[THOUGHT_04_04]**

- The lack of a small, provably sufficient set of candidate slopes via naive enumeration forces us to exploit smoothness or convexity of the objective across $r$.


---

**[CHAIN_05]**

Leveraging convexity to minimize the worst repaint

**[THOUGHT_05_01]**

- For a fixed $r$, define $d_i(r) = x[i] - r \cdot i$. Let $U(r) = \max_i d_i(r)$ and $L(r) = \min_i d_i(r)$. The spread is $S(r) = U(r) - L(r)$. The minimal worst repaint for slope $r$ is
$$
M(r) = \left\lceil \frac{S(r)}{2} \right\rceil.
$$
This is because the interval of feasible $s$ to keep max deviation $\le M$ is the intersection of $[d_i(r) - M, d_i(r) + M]$, which is non-empty iff $S(r) \le 2M$.


**[THOUGHT_05_02]**

- Each $d_i(r)$ is an affine function of $r$. $U(r)$ is the pointwise maximum of affine functions, hence convex. $-L(r)$ is also convex. Therefore $S(r) = U(r) - L(r)$ is convex and piecewise linear in real $r$. Consequently, $M(r)$ is minimized at a “valley” of $S(r)$.


**[THOUGHT_05_03]**


- To find the leftmost integer $r_0$ minimizing $S(r)$ over integers, observe that $S(r)$ is piecewise-linear and convex.  
For any integer $r$, the forward difference $\Delta S(r) = S(r+1) - S(r)$ is non-decreasing.  

- To locate the leftmost minimizer, perform a binary-like discrete convex search by comparing $S(\text{mid})$ and $S(\text{mid}+1)$.  
If $S(\text{mid}) \le S(\text{mid}+1)$, the leftmost minimizer lies at or to the left of $\text{mid}$; otherwise, it lies to the right.  

- Each evaluation requires computing $d_i(r) = x[i] - r \cdot i$ for all $i$, costing $O(n)$.  

- **Note on implementation:** In 64-bit arithmetic, the product $r \cdot i$ may overflow for big $|x[i]|$, $|r|$, or $i$. Using arbitrary-precision integers (Python `int`) or 128-bit intermediates (such as `__int128` in C++) to guarantee accuracy.


**[THOUGHT_05_04]**

- Once $r_0$ is found, let $S_0 = S(r_0)$. The globally minimal worst repaint is
$$
M^* = \left\lceil \frac{S_0}{2} \right\rceil.
$$
This concludes the first layer for at least one slope. However, there may be multiple integer $r$ that achieve the same minimal $M^*$ due to parity considerations.


**[THOUGHT_05_05]**

  
- Let $S_0 = S(r_0)$. Any integer slope $r$ satisfying $S(r) \le S_0 + 1$ (accounting for ceiling in $M = \lceil S/2 \rceil$) achieves the minimal worst repaint $M^*$.  

- Because $S(r)$ is convex and piecewise-linear, all such $r$ form a contiguous interval around $r_0$.  
The correct procedure is to expand leftward and rightward from $r_0$ until $S(r) > S_0 + 1$, collecting all integer slopes that still meet the bound.  

- There is no fixed upper limit like "$\le4$" on the number of valid slopes; the width of the interval depends on the data, though it is typically small in practical cases.  



**[THOUGHT_05_06]**

- If $n = 1$, then $S(r) \equiv 0$ for all $r$. Hence $M^* = 0$ for any $r$, and the optimal $y$ is trivially $[x_0]$ with $C = 0$. No slope search is needed, and lexicographic considerations do not alter the single-element result.


---

**[CHAIN_06]**

Minimizing weighted $L_1$ under the optimal Chebyshev bound

**[THOUGHT_06_01]**

- With $d_i = x[i] - r \cdot i$, let $\min D = \min_i d_i$, $\max D = \max_i d_i$, and $S = \max D - \min D$. The per-slope optimal worst repaint is $M = \lceil S/2 \rceil$. The feasible $s$ that achieve this $M$ are exactly the integers in
$$
[S_{\min}, S_{\max}] = [\max D - M, \min D + M].
$$


**[THOUGHT_06_02]**

-  The weighted $L_1$ objective $C(s) = \sum_i p[i] \cdot |d_i - s|$ is minimized by any weighted median of the multiset $\{d_i\}$ with weights $\{p[i]\}$. To enforce lexicographic minimality within a fixed $r$, we take the lower weighted median (smallest $s$ among minimizers of $C$ without constraints).


**[THOUGHT_06_03]**

-  Under the Chebyshev-optimal constraint $s \in [S_{\min}, S_{\max}]$, the optimal $s^*$ is
$$
s^* = \text{clamp}(m_{\text{low}}, S_{\min}, S_{\max}),
$$
where $m_{\text{low}}$ is the weighted lower median. This follows from convexity: $C(s)$ is convex in $s$, so minimizing over an interval reduces to clamping a global minimizer to that interval.


**[THOUGHT_06_04]**

- If multiple $s$ in $[S_{\min}, S_{\max}]$ give the same minimal $C$, the smallest such $s$ yields the lexicographically smallest $y$ for that $r$. Choosing the lower weighted median and then clamping achieves this automatically.


**[THOUGHT_06_05]**



- For a fixed slope $r$, define $di=x[i]−r⋅i$ and identify $minD$ and $maxD$ to determine the feasible interval $[Smin,Smax]$. The weighted $L1$ cost $C(s)=∑p[i]⋅|di−s|$ is minimized by the weighted lower median within this interval. Analytically, the median balances cumulative weights around half the total weight, ensuring minimal $C$.




---

**[CHAIN_07]**

Combining candidate slopes and breaking ties

**[THOUGHT_07_01]**



- After finding $r_0$, expand left and right from $r_0$ until $S(r)$ exceeds $S_0+1$ to collect all integer slopes achieving $M^*$. Deduplicate any repeated $r$ values to avoid redundant evaluations.  




**[THOUGHT_07_02]**

- For each $r$ in the filtered candidate set, compute $d_i$, $(\min D, \max D)$, the per-slope $M$, the feasible interval $[S_{\min}, S_{\max}]$, the weighted lower median $m_{\text{low}}$, then $s^* = \text{clamp}(m_{\text{low}}, S_{\min}, S_{\max})$. Construct $y[i] = s^* + r \cdot i$.


**[THOUGHT_07_03]**

Computing and verifying metrics: For each candidate, compute
- $M = \max_i |x[i] - y[i]|$,
- $C = \sum_i p[i] \cdot |x[i] - y[i]|$.
This validates consistency between the theoretical $M$ and the constructed $y$ and produces comparable tuples across candidates.


**[THOUGHT_07_04]**

- Among all candidates, select the one minimizing the tuple $(M, C, s, r)$. This tuple ordering exactly encodes the problem’s priorities: worst repaint first, then weighted cost, then lexicographic order via $s$, then $r$.


---

**[CHAIN_08]**

Analyzing complexity, space, and correctness

**[THOUGHT_08_01]**



- The discrete convex search requires $O(\log R)$ evaluations, where $R$ is the integer search range for $r$ (bounded by input magnitudes of $x[i]$ and $n$).  
Each evaluation costs $O(n)$ to compute $U(r)$ and $L(r)$.  

- After locating $r_0$, slopes achieving the same minimal $M^*$ are found by expanding outward until $S(r) > S_0 + 1$.  
Let $K$ denote the number of such candidate slopes.  

- Each candidate slope requires $O(n \log n)$ to compute the weighted median and associated cost metrics, yielding a total complexity of $O(N \log R + K \cdot N \log N)$ across all test cases (with total beads $N$).  
- In practice, $K$ is small but not guaranteed constant; the expansion process determines it explicitly.  





**[THOUGHT_08_02]**

Space complexity: Storing arrays and temporary pairs $(d_i, p[i])$ requires $O(n)$ space per test case. No extra asymptotic memory is required.


**[THOUGHT_08_03]**

Correctness summary for Layer 1: $S(r) = U(r) - L(r)$ is convex and piecewise linear; minimizing $S(r)$ over integers by discrete convex search yields $r_0$. The worst repaint at slope $r$ is $M(r) = \lceil S(r)/2 \rceil$, hence the global minimum $M^*$ is attained at $r_0$. Parity implies all $r$ with $S(r) \le T$ achieve $M^*$ and lie near $r_0$ for $n \ge 2$.


**[THOUGHT_08_04]**

Correctness summary for Layer 2 and 3: For fixed $r$, $C(s)$ is convex; unconstrained minimizers are weighted medians. Restricting to the interval $[S_{\min}, S_{\max}]$ preserves convexity and yields the clamped lower median as the minimizer, which also ensures lexicographic minimality for that slope. Finally, comparing $(M, C, s, r)$ across candidate slopes ensures the global tie-breaking is respected.


---

**[CHAIN_09]**

Managing numeric robustness and implementation pitfalls

**[THOUGHT_09_01]**


- Values $x[i]$, $s$, $r$, and $i$ can be large, and intermediate computations like $r \cdot i$ and $s + r \cdot i$ may overflow standard 64-bit integers.  

- To ensure numeric robustness:  

- **In C++ or similar fixed-width languages:** use 128-bit intermediates (`__int128`) when evaluating $d_i(r) = x[i] - r \cdot i$ and during accumulation of the total cost  
  $C = \sum p[i] \cdot |x[i] - y[i]|$.  

- **In Python:** native integers (`int`) already provide arbitrary precision, so no special handling is required.  

- Always accumulate cost $C$ in a wide integer type, since even moderate values of $p[i]$ and deviations can yield totals exceeding 64-bit limits.  



**[THOUGHT_09_02]**

-  For large inputs, prefer buffered reading and writing. Avoid creating many intermediate lists; process in-place where possible. Ensure outputs match the exact formatting: first line $M$ and $C$, second line the sequence $y$.


**[THOUGHT_09_03]**

Edge cases in computation:
- Handle $n = 1$ early by returning $M = 0$, $C = 0$, and $y = [x_0]$.
- Ensure the ceiling operation for $M = \lceil S/2 \rceil$ is implemented as $M = (S + 1) // 2$ for integers.
- The weighted lower median must be defined as the smallest $s$ where cumulative weight reaches at least half (rounded up) of the total weight.


**[THOUGHT_09_04]**

- When forming candidate slopes, deduplicate $r$ values to avoid redundant work. Sorting pairs $(d_i, p[i])$ for weighted medians is deterministic and consistent with tie-breaking to the lower median.


---

**[CHAIN_10]**

Validating with examples and constructing additional checks

**[THOUGHT_10_01]**

- For $n=1$, with any $x[0]$ and weight $p[0]$, the sequence consists of a single value. Analysis shows that the maximum deviation $M$ is zero, and the weighted distance $C$ is zero. This case highlights the trivial nature of sequences with a single element and the absence of slope influence.


**[THOUGHT_10_02]**

- Sample 3: $x=[10,0,10]$, $p=[1,100,1]$. For the optimal candidate slope(s), di align so that $S=10$ and $M=⌈10/2⌉=5$. The feasible interval for $s$ is $[5,5]$. The weighted lower median under any candidate r that preserves M collapses to s=5, giving $y=[5,5,5]$, $M=5$, and $C=1⋅5 + 100⋅5 + 1⋅5 = 510$.


**[THOUGHT_10_03]**

Additional tie case: $x = [0, 2]$, $p = [1, 1]$.
- With $r = 2$, $s = 0$, $y = [0, 2]$ gives $M = 0$, $C = 0$.
- With $r = 0$, $s = 1$, $y = [1, 1]$ gives $M = 1$, $C = 2$.
The first is strictly better in $M$. This simple check confirms the priority order and rejects “average” choices that trade $M$ for $C$.


---

**[CHAIN_11]**

Reflecting on failed attempts and the final improvements

**[THOUGHT_11_01]**

- The decision space for $(s, r)$ is unbounded and two-dimensional; even aggressive bounding guesses are not guaranteed and are too large. The naive approach is super-polynomial in the numeric ranges involved and cannot handle $N \le 2 \cdot 10^5$.


**[THOUGHT_11_02]**

- Scanning $r$ across a large numeric interval is linear in the range size and thus infeasible. Generating $O(n^2)$ pairwise candidate slopes is also infeasible at the input scale.


**[THOUGHT_11_03]**

- Recognizing that $S(r)$ is convex and piecewise linear allows a discrete convex search to find a leftmost minimizer in $O(n \log R)$. This leap transforms an intractable enumeration into a logarithmic-time search over the slope space.


**[THOUGHT_11_04]**

- For fixed $r$, the constrained weighted $L_1$ minimization collapses to clamping the lower weighted median to a small interval, yielding both cost optimality and lexicographic minimality. This avoids any continuous optimization and relies on robust order-statistics.


**[THOUGHT_11_05]**

- The final pipeline is: find $r_0$ via discrete convex search, expand to a constant-sized set of nearby $r$ preserving $M^*$, compute $s^*$ via clamped lower weighted median per $r$, then select by $(M, C, s, r)$. Each refinement directly addresses a specific failure of earlier attempts while improving both correctness guarantees and efficiency.

---

**[RESPONSE]**

**1. Problem Understanding**

We are given two integer arrays:
- $x[0..n-1]$: current glows of beads,
- $p[0..n-1]$: positive repaint costs per unit for each bead.

We must choose integers $s$ and $r$ to form a final arithmetic progression
$y[i] = s + r \cdot i$ for $0 \le i < n$. The optimization follows a strict
priority:
1) Minimize the worst repaint (Chebyshev norm)
$$
M = \max_i |x[i] - y[i]|.
$$
2) Among those, minimize the total weighted repaint cost (weighted $L_1$)
$$
C = \sum_i p[i] \cdot |x[i] - y[i]|.
$$
3) If still tied, choose the lexicographically smallest $y$ sequence.
For arithmetic progressions, this is equivalent to smaller $s$, and if $s$
ties, smaller $r$.

Key observations:
- For fixed $r$, define $d_i(r) = x[i] - r \cdot i$. Then
  $|x[i] - y[i]| = |d_i(r) - s|$.
- Define the spread
$$
S(r) = \max_i d_i(r) - \min_i d_i(r).
$$
  The minimal worst repaint at slope $r$ is
$$
M(r) = \left\lceil \frac{S(r)}{2} \right\rceil.
$$
- $S(r)$ is convex and piecewise linear in real $r$; hence we can find a
  leftmost integer minimizer via discrete convex search.
- For fixed $r$, the weighted $L_1$-optimal $s$ is a weighted median of
  $\{d_i(r)\}$ with weights $\{p[i]\}$. Under the constraint that $s$ must
  achieve the optimal $M(r)$, clamp the lower weighted median to the feasible
  interval.

**2. Optimal Approach**

We solve in three layers using convexity and weighted median properties.

- Step 1: Minimize worst repaint across slopes.
  - For fixed $r$, compute $S(r)$ by scanning $d_i(r) = x[i] - r \cdot i$.
  - $S(r)$ is convex; find the leftmost integer minimizer $r_0$ using discrete
    convex (unimodal) search by comparing $S(\text{mid})$ and $S(\text{mid}+1)$.
  - Let $S_0 = S(r_0)$ and $M^* = \lceil S_0 / 2 \rceil$.
  - Due to parity, all integer $r$ with $S(r) \le T$ achieve the same $M^*$,
    where $T = S_0$ if $S_0$ is even, else $T = S_0 + 1$.
  - For $n \ge 2$, these $r$ lie within $\{r_0 - 1, r_0, r_0 + 1, r_0 + 2\}$.

- Step 2: Minimize weighted cost for each candidate slope.
  - For each such $r$, compute $d_i$, $\min D$, $\max D$, and $S = \max D -
    \min D$.
  - The per-slope optimal worst repaint is $M = \lceil S/2 \rceil$.
  - Feasible $s$ are integers in $[S_{\min}, S_{\max}]$ with
    $S_{\min} = \max D - M$ and $S_{\max} = \min D + M$.
  - The weighted $L_1$ optimal $s$ is the lower weighted median of $\{d_i\}$
    clamped to $[S_{\min}, S_{\max}]$.

- Step 3: Break ties across slopes.
  - For each candidate $r$, compute the resulting $M$, $C$, and $s$, construct
    $y$, then select the best by the tuple $(M, C, s, r)$ to enforce priority
    order and lexicographic tie-breaking.

Corner case:
- If $n = 1$, then $S(r) \equiv 0$ for any $r$; return $M = 0$, $C = 0$,
  and $y = [x_0]$.

**3. Code**
```python
import sys

INF = 10 ** 30


def span_at_slope(values: list, slope: int) -> int:
    """
    Compute the spread S(slope) = max_i (values[i] - slope*i) -
    min_i (values[i] - slope*i) in O(n).

    Args:
        values: List of integers representing artifact values.
        slope: Integer slope to apply for linear adjustment.

    Returns:
        The spread of values after subtracting slope*i from each value.
    """
    min_val = INF
    max_val = -INF
    for i, val in enumerate(values):
        adjusted = val - slope * i
        if adjusted < min_val:
            min_val = adjusted
        if adjusted > max_val:
            max_val = adjusted
    return max_val - min_val


def leftmost_argmin_span(values: list) -> int:
    """
    Find the leftmost integer slope that minimizes the spread using
    discrete convex search by comparing span(mid) and span(mid+1).

    Args:
        values: List of integers representing artifact values.

    Returns:
        The integer slope that minimizes the spread.
    """
    low = -2_000_000_010
    high = 2_000_000_010
    while low < high:
        mid = (low + high) // 2
        s_mid = span_at_slope(values, mid)
        s_next = span_at_slope(values, mid + 1)
        if s_mid <= s_next:
            high = mid
        else:
            low = mid + 1
    return low


def weighted_lower_median(sorted_pairs: list, total_weight: int) -> int:
    """
    Compute the weighted lower median from sorted (value, weight) pairs.

    Args:
        sorted_pairs: List of (value, weight) tuples sorted by value.
        total_weight: Total weight sum.

    Returns:
        The smallest value such that cumulative weight >= (total_weight + 1)//2.
    """
    needed = (total_weight + 1) // 2
    accumulated = 0
    for value, weight in sorted_pairs:
        accumulated += weight
        if accumulated >= needed:
            return value
    # Fallback: return largest value if cumulative weight never reaches needed
    return sorted_pairs[-1][0]


def best_given_slope(values: list, weights: list, slope: int):
    """
    For a fixed slope, compute the optimal starting value s (clamped lower
    weighted median), the realized maximum deviation, total weighted cost,
    and final sequence.

    Args:
        values: List of original values.
        weights: List of integer weights for each value.
        slope: Integer slope to apply.

    Returns:
        Tuple of (s, max_deviation, total_weighted_cost, final_sequence)
    """
    n = len(values)
    # Adjust values by removing linear slope component
    adjusted = [values[i] - slope * i for i in range(n)]
    min_adj = min(adjusted)
    max_adj = max(adjusted)

    # Maximum half spread (ceiling)
    half_spread = (max_adj - min_adj + 1) // 2
    s_min = max_adj - half_spread
    s_max = min_adj + half_spread

    # Compute weighted lower median
    value_weight_pairs = sorted(zip(adjusted, weights))
    total_weight = sum(weights)
    median = weighted_lower_median(value_weight_pairs, total_weight)

    # Clamp s within feasible range
    if median < s_min:
        s = s_min
    elif median > s_max:
        s = s_max
    else:
        s = median

    # Construct final sequence and compute deviations and cost
    final_sequence = [s + slope * i for i in range(n)]
    max_diff = 0
    total_cost = 0
    for i in range(n):
        diff = abs(values[i] - final_sequence[i])
        max_diff = max(max_diff, diff)
        total_cost += weights[i] * diff

    return s, max_diff, total_cost, final_sequence


def main() -> None:
    """
    Solve all test cases:
    Choose slope and start s to minimize (maximum deviation, total cost)
    and return the lexicographically smallest sequence.
    """
    data = list(map(int, sys.stdin.buffer.read().split()))
    t = data[0]
    idx = 1
    out_lines = []

    for _ in range(t):
        n = data[idx]
        idx += 1
        values = data[idx:idx + n]
        idx += n
        weights = data[idx:idx + n]
        idx += n

        if n == 1:
            # Single element: no slope needed
            out_lines.append("0 0")
            out_lines.append(str(values[0]))
            continue

        # Find slope minimizing spread
        best_slope = leftmost_argmin_span(values)
        spread = span_at_slope(values, best_slope)
        threshold = spread if spread % 2 == 0 else spread + 1

        # Consider slopes around the best candidate
        candidate_slopes = [best_slope - 1, best_slope,
                            best_slope + 1, best_slope + 2]
        valid_slopes = []
        seen = set()
        for slope in candidate_slopes:
            if slope in seen:
                continue
            seen.add(slope)
            if span_at_slope(values, slope) <= threshold:
                valid_slopes.append(slope)

        # Choose slope and start s minimizing (M, total_cost, s, slope)
        best_key = None
        best_sequence = None
        for slope in valid_slopes:
            s, m_val, cost, sequence = best_given_slope(values, weights, slope)
            key = (m_val, cost, s, slope)
            if best_key is None or key < best_key:
                best_key = key
                best_sequence = sequence

        m_val, cost, s, slope = best_key
        out_lines.append(f"{m_val} {cost}")
        out_lines.append(" ".join(map(str, best_sequence)))

    sys.stdout.write("\n".join(out_lines))


if __name__ == "__main__":
    main()
```

**4. Code Explanation**

- span_at_slope:
  Computes $S(r) = \max_i (x[i] - r i) - \min_i (x[i] - r i)$ in $O(n)$ by a
  single scan, tracking the min and max of $d_i(r)$.

- leftmost_argmin_span:
  Uses discrete convex (unimodal) search to find the leftmost integer $r$ that
  minimizes $S(r)$. It compares $S(\text{mid})$ and $S(\text{mid}+1)$ to decide
  which half to keep, running in $O(\log R)$ iterations.

- weighted_lower_median:
  Given sorted $(d_i, p_i)$ pairs, returns the lower weighted median: the
  smallest value whose cumulative weight reaches at least half the total
  (rounded up). This minimizes the weighted $L_1$ objective unconstrained.

- best_given_slope:
  For fixed $r$, constructs $d_i = x[i] - r i$, computes spread and per-slope
  optimal $M = \lceil S/2 \rceil$. The feasible $s$ are integers in
  $[S_{\min}, S_{\max}]$. It takes the lower weighted median and clamps it
  to this interval, builds $y$, and computes the realized $M$ and $C$.

- main:
  Handles $n = 1$ as a trivial case. Otherwise, it finds $r_0$ minimizing
  $S(r)$, sets threshold $T$ (parity-adjusted), forms up to four candidate
  slopes around $r_0$ that keep $S(r) \le T$, evaluates each candidate to
  obtain $(M, C, s, y)$, and selects the best by the tuple $(M, C, s, r)$.
  Finally, it prints $M$, $C$, and the sequence $y$.

**5. Time and Space Complexity**

- Discrete convex search for the leftmost minimizer $r_0$:
  $O(n \log R)$ evaluations of $S(r)$, where $R$ is the numeric range of $r$
  (about $4 \cdot 10^9$), so approximately $32$–$62$ iterations.

- Per candidate slope (at most 4):
  - Sorting $(d_i, p_i)$ to find the weighted lower median: $O(n \log n)$.
  - Building $y$ and computing $(M, C)$: $O(n)$.

Across all test cases with total $N$, total time is
$O(N \log R + N \log N)$ and space is $O(N)$.

**6. Conclusion**

This solution exploits convexity of the spread $S(r)$ to find a near-minimum
set of slopes and uses the weighted median property to minimize the cost under
the optimal worst repaint. By clamping the lower weighted median to the
feasible interval and selecting globally by $(M, C, s, r)$, we respect the
priority order and lexicographic tie-breaking. The method is efficient and
robust for the problem’s constraints and handles edge cases like $n = 1$
gracefully.